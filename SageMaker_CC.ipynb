{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35dd3a23-c1d0-4565-ad45-48d226c34a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers \"datasets[s3]==2.18.0\" \"sagemaker>=2.190.0\" \"huggingface_hub[cli]\" --upgrade --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "799d23c5-0b5f-49ea-b657-cf82aa75bb0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7dfe94f-3f6c-46cf-8f4a-5e44c3c1bd9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::590183788292:role/service-role/AmazonSageMaker-ExecutionRole-20240330T201472\n",
      "sagemaker bucket: sagemaker-us-east-1-590183788292\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b66bcb8b-d8a1-4774-ad43-3da615e7d15a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system', 'content': 'You are...'},\n",
       "  {'role': 'user', 'content': '...'},\n",
       "  {'role': 'assistant', 'content': '...'}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "168cda87-9497-4463-8fe0-6555efb012e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<prompt text>', 'completion': '<ideal generated text>'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0996544-38e8-4a65-b18b-fda796ba5299",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ab8a066cd843a7801844b672fb6740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21.8M/21.8M [00:00<00:00, 192MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28fa40923924d51a63c97f374782896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db0fa7dda844a19b08d07d6f68838bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_29301050_1 (train_name VARCHAR, train_number VARCHAR)', 'role': 'system'}, {'content': 'how many trains have the number 99808', 'role': 'user'}, {'content': 'SELECT COUNT(train_name) FROM table_29301050_1 WHERE train_number = 99808', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(12500))\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "# split dataset into 10,000 training samples and 2,500 test samples\n",
    "dataset = dataset.train_test_split(test_size=2500/12500)\n",
    "\n",
    "print(dataset[\"train\"][345][\"messages\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a029b7f-96f7-4d11-8434-dc319548ea60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fsspec/registry.py:273: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8e5334467340b09204e8d2885a0084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b88ab3ce9244e4979d8b11a1aa61f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to:\n",
      "s3://sagemaker-us-east-1-590183788292/datasets/text-to-sql/train_dataset.json\n",
      "https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-590183788292/?region=us-east-1&prefix=datasets/text-to-sql/\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "training_input_path = f's3://{sess.default_bucket()}/datasets/text-to-sql'\n",
    "\n",
    "# save datasets to s3\n",
    "dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(f\"{training_input_path}/train_dataset.json\")\n",
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={training_input_path.split('/', 3)[-1]}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebc797eb-384c-49f5-aee3-5372b4d695f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "  ### SCRIPT PARAMETERS ###\n",
    "  'dataset_path': '/opt/ml/input/data/training/train_dataset.json', # path where sagemaker will save training dataset\n",
    "  'model_id': \"codellama/CodeLlama-7b-hf\",           # or `mistralai/Mistral-7B-v0.1`\n",
    "  'max_seq_len': 3072,                               # max sequence length for model and packing of the dataset\n",
    "  'use_qlora': True,                                 # use QLoRA model\n",
    "  ### TRAINING PARAMETERS ###\n",
    "  'num_train_epochs': 3,                             # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                  # batch size per device during training\n",
    "  'gradient_accumulation_steps': 4,                  # number of steps before performing a backward/update pass\n",
    "  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n",
    "  'optim': \"adamw_torch_fused\",                      # use fused adamw optimizer\n",
    "  'logging_steps': 10,                               # log every 10 steps\n",
    "  'save_strategy': \"epoch\",                          # save checkpoint every epoch\n",
    "  'learning_rate': 2e-4,                             # learning rate, based on QLoRA paper\n",
    "  'bf16': True,                                      # use bfloat16 precision\n",
    "  'tf32': True,                                      # use tf32 precision\n",
    "  'max_grad_norm': 0.3,                              # max gradient norm based on QLoRA paper\n",
    "  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n",
    "  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n",
    "  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n",
    "  'output_dir': '/tmp/tun',                          # Temporary output directory for model checkpoints\n",
    "  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e041a1e-c12d-4367-8b9e-c8e46d028207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'codellama-7b-hf-text-to-sql-exp1'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_sft.py',    # train script\n",
    "    source_dir           = '.https://github.com/philschmid/llm-sagemaker-sample/blob/main/scripts/trl',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    environment          = {\n",
    "                            \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "                            # \"HF_TOKEN\": \"REPALCE_WITH_YOUR_TOKEN\" # huggingface token to access gated models, e.g. llama 2\n",
    "                            },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68d34189-8274-4238-a4bf-2ab5e4ef97c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_dir = \"./llm-sagemaker-sample/scripts/trl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc078cc6-6fd1-420f-9875-18668bf99690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No file named \"run_sft.py\" was found in directory \".https://github.com/philschmid/llm-sagemaker-sample/blob/main/scripts/trl\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m: training_input_path}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# starting the train job with our uploaded datasets as input\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mhuggingface_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1335\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;129m@runnable_by_pipeline\u001b[39m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     experiment_config: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1278\u001b[0m ):\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train a model using the input training dataset.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[1;32m   1281\u001b[0m \u001b[38;5;124;03m    The API calls the Amazon SageMaker CreateTrainingJob API to start\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;124;03m        :class:`~sagemaker.workflow.pipeline_context.PipelineSession`\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1337\u001b[0m     experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m _TrainingJob\u001b[38;5;241m.\u001b[39mstart_new(\u001b[38;5;28mself\u001b[39m, inputs, experiment_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:3515\u001b[0m, in \u001b[0;36mFramework._prepare_for_training\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m   3507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3508\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set hyperparameters needed for training. This method will also validate ``source_dir``.\u001b[39;00m\n\u001b[1;32m   3509\u001b[0m \n\u001b[1;32m   3510\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;124;03m            constructor if applicable.\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3515\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFramework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_set_debugger_configs()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:913\u001b[0m, in \u001b[0;36mEstimatorBase._prepare_for_training\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdependencies:\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;66;03m# validate source dir will raise a ValueError if there is something wrong with\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;66;03m# the source directory. We are intentionally not handling it because this is a\u001b[39;00m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# critical error.\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dir\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_pipeline_variable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dir)\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dir\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    912\u001b[0m     ):\n\u001b[0;32m--> 913\u001b[0m         \u001b[43mvalidate_source_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;66;03m# if we are in local mode with local_code=True. We want the container to just\u001b[39;00m\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;66;03m# mount the source dir instead of uploading to S3.\u001b[39;00m\n\u001b[1;32m    917\u001b[0m     local_code \u001b[38;5;241m=\u001b[39m get_config_value(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal.local_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/fw_utils.py:214\u001b[0m, in \u001b[0;36mvalidate_source_dir\u001b[0;34m(script, directory)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m directory:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, script)):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo file named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m was found in directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(script, directory)\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: No file named \"run_sft.py\" was found in directory \".https://github.com/philschmid/llm-sagemaker-sample/blob/main/scripts/trl\"."
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f174622-f14f-4809-b33d-a4e73fb0cc66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-590183788292/codellama-7b-hf-text-to-sql-exp1-2024-03-30-16-10-55-729/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_uri = huggingface_estimator.model_data\n",
    "if s3_uri.startswith(\"s3://\"):\n",
    "    console_link = s3_uri.replace(\"s3://\", \"https://s3.console.aws.amazon.com/s3/buckets/\")\n",
    "    print(console_link)\n",
    "else:\n",
    "    print(\"The provided URI does not start with 's3://'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1c5b4f7-599c-4fb7-9ae7-8d95f456bf52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.0-gpu-py310-cu121-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.4.0\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fa1ae37-c62c-4344-a026-bfa72e56d68f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example training_data definition\n",
    "training_data = \"s3://sagemaker-us-east-1-590183788292/datasets/text-to-sql/train_dataset.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d9579cc-8e92-4058-8665-6b200d6c211b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No file named \"run_sft.py\" was found in directory \".https://github.com/philschmid/llm-sagemaker-sample/blob/main/scripts/trl\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming huggingface_estimator is properly defined and trained\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Fit the estimator (train the model)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mhuggingface_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Access the model data after training\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model_s3_path \u001b[38;5;241m=\u001b[39m huggingface_estimator\u001b[38;5;241m.\u001b[39mmodel_data\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1335\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;129m@runnable_by_pipeline\u001b[39m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     experiment_config: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1278\u001b[0m ):\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train a model using the input training dataset.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[1;32m   1281\u001b[0m \u001b[38;5;124;03m    The API calls the Amazon SageMaker CreateTrainingJob API to start\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;124;03m        :class:`~sagemaker.workflow.pipeline_context.PipelineSession`\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1337\u001b[0m     experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m _TrainingJob\u001b[38;5;241m.\u001b[39mstart_new(\u001b[38;5;28mself\u001b[39m, inputs, experiment_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:3515\u001b[0m, in \u001b[0;36mFramework._prepare_for_training\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m   3507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3508\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set hyperparameters needed for training. This method will also validate ``source_dir``.\u001b[39;00m\n\u001b[1;32m   3509\u001b[0m \n\u001b[1;32m   3510\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;124;03m            constructor if applicable.\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3515\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFramework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_set_debugger_configs()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:913\u001b[0m, in \u001b[0;36mEstimatorBase._prepare_for_training\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdependencies:\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;66;03m# validate source dir will raise a ValueError if there is something wrong with\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;66;03m# the source directory. We are intentionally not handling it because this is a\u001b[39;00m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# critical error.\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dir\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_pipeline_variable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dir)\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dir\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    912\u001b[0m     ):\n\u001b[0;32m--> 913\u001b[0m         \u001b[43mvalidate_source_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;66;03m# if we are in local mode with local_code=True. We want the container to just\u001b[39;00m\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;66;03m# mount the source dir instead of uploading to S3.\u001b[39;00m\n\u001b[1;32m    917\u001b[0m     local_code \u001b[38;5;241m=\u001b[39m get_config_value(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal.local_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/fw_utils.py:214\u001b[0m, in \u001b[0;36mvalidate_source_dir\u001b[0;34m(script, directory)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m directory:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, script)):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo file named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m was found in directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(script, directory)\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: No file named \"run_sft.py\" was found in directory \".https://github.com/philschmid/llm-sagemaker-sample/blob/main/scripts/trl\"."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Assuming huggingface_estimator is properly defined and trained\n",
    "# Fit the estimator (train the model)\n",
    "huggingface_estimator.fit(training_data)\n",
    "\n",
    "# Access the model data after training\n",
    "model_s3_path = huggingface_estimator.model_data\n",
    "\n",
    "# Define other necessary parameters\n",
    "role = \"your-sagemaker-role\"\n",
    "llm_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.0-gpu-py310-cu121-ubuntu20.04\"\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "    # Your configuration parameters here\n",
    "}\n",
    "\n",
    "# Create HuggingFaceModel with the image URI\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=role,\n",
    "    image_uri=llm_image,\n",
    "    model_data=model_s3_path,\n",
    "    env=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb5ce5c1-655c-46dc-a244-c36a9b5d89dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Deploy model to an endpoint\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mllm_model\u001b[49m\u001b[38;5;241m.\u001b[39mdeploy(\n\u001b[1;32m      4\u001b[0m   initial_instance_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m   instance_type\u001b[38;5;241m=\u001b[39minstance_type,\n\u001b[1;32m      6\u001b[0m   container_startup_health_check_timeout\u001b[38;5;241m=\u001b[39mhealth_check_timeout, \u001b[38;5;66;03m# 10 minutes to give SageMaker the time to download the model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to give SageMaker the time to download the model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70ab772a-8db2-4f2a-8727-72d7ace130f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4247f8acefaa4317a2be225f2e53c05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fa9f202b1943938f28a7131060612c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a868aca7d9452c8abfb698062277b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c16c21a51749ce86ae398b212fc13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6761d7047eb46e2baefed13aa98a010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the CodeLlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Find the program which most number of students are enrolled in. List both the id and the summary.', 'role': 'user'}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(random_sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 26\u001b[0m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 14\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(sample):\n\u001b[1;32m     13\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(sample, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241m.\u001b[39mpredict({\n\u001b[1;32m     15\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m     16\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_full_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|im_end|>\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     21\u001b[0m       }\n\u001b[1;32m     22\u001b[0m     })\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "\n",
    "# Load the test dataset from s3\n",
    "S3Downloader.download(f\"{training_input_path}/test_dataset.json\", \".\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\",split=\"train\")\n",
    "random_sample = test_dataset[345]\n",
    "\n",
    "def request(sample):\n",
    "    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = llm.predict({\n",
    "      \"inputs\": prompt,\n",
    "      \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"stop\": [\"<|im_end|>\"],\n",
    "      }\n",
    "    })\n",
    "    return {\"role\": \"assistant\", \"content\": outputs[0][\"generated_text\"].strip()}\n",
    "\n",
    "print(random_sample[\"messages\"][1])\n",
    "request(random_sample[\"messages\"][:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95aa73e4-4721-4b1c-857c-1d277b2c7ded",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# iterate over eval dataset and predict\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m tqdm(test_dataset\u001b[38;5;241m.\u001b[39mshuffle()\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(number_of_eval_samples))):\n\u001b[0;32m---> 14\u001b[0m     success_rate\u001b[38;5;241m.\u001b[39mappend(\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# compute accuracy\u001b[39;00m\n\u001b[1;32m     17\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(success_rate)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(success_rate)\n",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(sample):\n\u001b[0;32m----> 4\u001b[0m     predicted_answer \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m predicted_answer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[29], line 14\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(sample):\n\u001b[1;32m     13\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(sample, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241m.\u001b[39mpredict({\n\u001b[1;32m     15\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m     16\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_full_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|im_end|>\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     21\u001b[0m       }\n\u001b[1;32m     22\u001b[0m     })\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(sample):\n",
    "    predicted_answer = request(sample[\"messages\"][:2])\n",
    "    if predicted_answer[\"content\"] == sample[\"messages\"][2][\"content\"]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "success_rate = []\n",
    "number_of_eval_samples = 1000\n",
    "# iterate over eval dataset and predict\n",
    "for s in tqdm(test_dataset.shuffle().select(range(number_of_eval_samples))):\n",
    "    success_rate.append(evaluate(s))\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = sum(success_rate)/len(success_rate)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca831a-ef8f-42bd-a5ea-ea854511b18f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
